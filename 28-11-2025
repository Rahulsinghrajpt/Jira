Investigating the date column transformation logic. Reviewing the code that handles date/week column processing:
Reviewing the exact logic that processes week columns:
Checking if there's a standardization function that also processes week columns:
Checking the column standardizer for week column handling:
Checking the standardization function for week column processing:
Problem
The code in process_with_pandas (lines 687-710) finds any column containing "week" and renames the first match to "Date". This causes:
Columns like primeday_week, blackfriday_week are matched
The first match is renamed to "Date", creating a duplicate if "Date" already exists
Other "week" columns may be processed incorrectly
Root cause
# Line 688: This matches ANY column containing "week"week_columns_lower = [col for col in df_columns_lower if 'week' in col]
This matches:
week ✓ (intended)
primeday_week ✗ (unintended)
blackfriday_week ✗ (unintended)
Fix
Update the logic to:
Check if "Date" already exists — if yes, skip week processing
Only match a column named exactly "week" (case-insensitive), not columns containing "week"
Here's the corrected code:
# Around line 687-710 in lambda_function.py# Preserve original column names (data integrity requirement)original_columns = df.columns.tolist()# Create lowercase copy for checking only (DO NOT modify df.columns)df_columns_lower = df.columns.str.lower()# Check if "Date" column already existshas_date_column = 'date' in df_columns_lower# Only process week column if Date doesn't already existif not has_date_column:    # Find column named exactly "week" (case-insensitive), not columns containing "week"    week_col_index = None    original_week_col = None        for idx, col_lower in enumerate(df_columns_lower):        if col_lower == 'week':  # Exact match only            week_col_index = idx            original_week_col = original_columns[idx]            break        if week_col_index is not None and original_week_col:        logger.info(f"Found week column: {original_week_col}, renaming to 'Date'")        df = df.rename(columns={original_week_col: 'Date'})                # Update original_columns and df_columns_lower to reflect the rename        original_columns[week_col_index] = 'Date'        df_columns_lower = df.columns.str.lower()  # Recreate lowercase copy after rename                # Reformat date values to YYYY-MM-DD        try:            # Try parsing as date first            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')            # Format to YYYY-MM-DD            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')            # Replace NaT with empty string            df['Date'] = df['Date'].fillna('')            logger.info(f"Reformatted {len(df)} date values to YYYY-MM-DD format")        except Exception as e:            logger.warning(f"Error reformatting dates: {e}")    else:        logger.info("No 'week' column found (exact match), Date column may already exist or not needed")else:    logger.info("Date column already exists, skipping week column processing")
Changes
Check for existing "Date" column first
Match only exact "week" (case-insensitive), not columns containing "week"
Skip processing if "Date" already exists
Add logging for clarity
Testing
After applying the fix, verify:
Files with "Date" column → no duplicate Date columns
Files with "week" column → renamed to "Date"
Files with "primeday_week", "blackfriday_week" → left unchanged
Files with both "Date" and "week" → "Date" kept, "week" not processed
Should I create a patch file with this fix, or do you want to apply it directly?















Step 1: Create Lambda Zip Package
# Navigate to Lambda function directorycd C:\Users\anish_ksc\Desktop\local_repo\MikGorilla.AI\data_ingestion_pipeline\lambdas\mmm_dev_data_transfer# Create zip package with updated codeCompress-Archive -Path *.py -DestinationPath mmm_dev_data_transfer.zip -Force# Verify zip was createdTest-Path mmm_dev_data_transfer.zip# Should return: True
Step 2: Navigate to Terraform Directory
# Navigate to terraform directorycd C:\Users\anish_ksc\Desktop\local_repo\MikGorilla.AI\data_ingestion_pipeline\terraform
Step 3: Verify Terraform Workspace
# Check current workspace (should be 'default' for dev)terraform workspace show# If not 'default', switch to itterraform workspace select default
Step 4: Refresh AWS Credentials (if needed)
# Check if credentials are validaws sts get-caller-identity# If you get "ExpiredToken" error, refresh credentials:# Option 1: AWS SSOaws sso login --profile your-profile-name# Option 2: Set environment variables$env:AWS_ACCESS_KEY_ID = "your-access-key"$env:AWS_SECRET_ACCESS_KEY = "your-secret-key"$env:AWS_SESSION_TOKEN = "your-session-token"  # If using temporary credentials
Step 5: Validate Terraform Configuration
# Validate syntaxterraform validate# Expected output: Success! The configuration is valid.
Step 6: Plan Deployment
# Generate deployment plan.\deploy.ps1 -Environment dev -Action plan
Review the plan:
aws_lambda_function.data_transfer should show source_code_hash change
aws_lambda_alias.data_transfer_live should show version update
No other resources should change
Expected output:
Plan: 0 to add, 2 to change, 0 to destroy.
Step 7: Apply Deployment
# Deploy changes to dev environment.\deploy.ps1 -Environment dev -Action apply
During apply:
Review the plan when prompted
Type yes to confirm
Wait for completion (typically 1-2 minutes)
Expected output:
Apply complete! Resources: 0 added, 2 changed, 0 destroyed.
Step 8: Verify Lambda Deployment
# Check Lambda function was updatedaws lambda get-function --function-name mmm_dev_data_transfer --query 'Configuration.{Version:Version, LastModified:LastModified}' --output table# Expected: LastModified should show current timestamp
Step 9: Trigger Data Ingestion Pipeline
Option A: Trigger via Step Function (Recommended)
# Get Step Function ARN for dev$sfnArn = "arn:aws:states:eu-west-1:931493483974:stateMachine:mmm_dev_data_ingestion_orchestrator"# Start execution$execution = aws stepfunctions start-execution `    --state-machine-arn $sfnArn `    --input '{}' | ConvertFrom-Json# Save execution ARN$executionArn = $execution.executionArn








The Solution: "Query-Before-Failure"
Instead of guessing the key format, we treat DynamoDB as the Source of Truth.

Step 1: Try Standard Format – Attempt a direct lookup using the "ideal" normalized key (e.g., bella_us#amazon).

Step 2: Discover Real Format – If Step 1 fails, Query DynamoDB using the client_id to see all records for that client.

Step 3: Case-Insensitive Matching – Scan those records to find the brand and retailer (e.g., matching bella-US even if we searched for bella_us).

Step 4: Use the "True" Key – Extract the exact key format found in the database and use it to complete the request.









Phase 1: Package Lambda Functions
Step 1: Navigate to Terraform Directory
cd C:\Users\anish_ksc\Desktop\local_repo\MikGorilla.AI\data_ingestion_pipeline\terraform

Step 2: Package data_transfer Lambda (Updated Code)

# Navigate to data_transfer Lambda directorycd ..\lambdas\mmm_dev_data_transfer

# Remove old ZIP if existsRemove-Item mmm_dev_data_transfer.zip -ErrorAction SilentlyContinue

# Create temporary directory$tempDir = New-TemporaryFile | ForEach-Object { Remove-Item $_; New-Item -ItemType Directory -Path $_ }

# Copy lambda functionCopy-Item lambda_function.py -Destination "$tempDir\lambda_function.py"

# Copy src directory (contains utils modules)Copy-Item ..\..\src -Destination "$tempDir\src" -Recurse -Force

# Create ZIPCompress-Archive -Path "$tempDir\*" -DestinationPath mmm_dev_data_transfer.zip -Force

# CleanupRemove-Item $tempDir -Recurse -Force

Step 3: Return to Terraform Directory
cd ..\..\terraform

Phase 2: Deploy Using Terraform
Step 4: Verify Terraform Workspace
# Check current workspace (should be 'default' for dev)
terraform workspace show
# If not 'default', switch to it
terraform workspace select default
Step 5: Run Terraform Plan (Review Changes)
.\deploy.ps1 -Environment dev -Action plan
Expected Output: Should show changes to aws_lambda_function.data_transfer resource (source_code_hash will change)
Step 6: Deploy Changes
.\deploy.ps1 -Environment dev -Action apply






2026-01-08T17:50:05.492+05:30
INIT_START Runtime Version: python:3.12.v101 Runtime Version ARN: arn:aws:lambda:eu-west-1::runtime:994aac32248ecf4d69d9f5e9a3a57aba3ccea19d94170a61d5ecf978927e1b0f
2026-01-08T17:50:08.282+05:30
[INFO] 2026-01-08T12:20:08.282Z Found credentials in environment variables.
2026-01-08T17:50:08.540+05:30
START RequestId: 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Version: $LATEST
2026-01-08T17:50:08.541+05:30
[INFO] 2026-01-08T12:20:08.540Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Lambda 2 started: madebygather/bella_us/national (brand_id=bella-US, brand_name=Bella US)
2026-01-08T17:50:08.599+05:30
[INFO] 2026-01-08T12:20:08.599Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ✓ Connected to DynamoDB table: mmm-dev-pipeline-infos (region: eu-west-1, environment: dev)
2026-01-08T17:50:08.618+05:30
[WARNING] 2026-01-08T12:20:08.618Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ⚠ Pipeline info not found with normalized key 'bella_us#national'. Querying DynamoDB to discover actual brand_retailer_key format...
2026-01-08T17:50:08.624+05:30
[INFO] 2026-01-08T12:20:08.624Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ✓ Discovered brand_retailer_key format from DynamoDB: madebygather/bella-US#national (brand_id=bella-US, retailer_id=national)
2026-01-08T17:50:08.624+05:30
[WARNING] 2026-01-08T12:20:08.624Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ⚠ Using discovered brand_retailer_key format: 'bella-US#national' (differs from normalized: 'bella_us#national'). RECOMMENDATION: Update DynamoDB record to use normalized format for consistency. Migration steps: 1) Update create_pipeline_info() to auto-normalize keys, 2) Run migration script, 3) Remove discovery logic.
2026-01-08T17:50:08.628+05:30
[WARNING] 2026-01-08T12:20:08.628Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ✓ Retrieved pipeline info using discovered key: madebygather/bella-US#national. ACTION REQUIRED: Migrate record to normalized format 'bella_us#national'. Migration steps: 1) Update create_pipeline_info() to auto-normalize keys, 2) Run migration script to update existing records, 3) Remove discovery logic from get_pipeline_info().
2026-01-08T17:50:08.629+05:30
[INFO] 2026-01-08T12:20:08.629Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Found last_data_updated: 2025-12-11 (will filter files older than this)
2026-01-08T17:50:08.629+05:30
[INFO] 2026-01-08T12:20:08.629Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Scanning: s3://prd-mm-vendor-sync/tracer/ for madebygather/bella_us/national
2026-01-08T17:50:08.708+05:30
[INFO] 2026-01-08T12:20:08.708Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Found 2 matching files for madebygather/bella_us/national:
2026-01-08T17:50:08.708+05:30
[INFO] 2026-01-08T12:20:08.708Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad 1. 2026-01-madebygather_bella_us_national_359.csv | Modified: 2026-01-08 08:53:50 UTC | Size: 9,316 bytes → SELECTED
2026-01-08T17:50:08.708+05:30
[INFO] 2026-01-08T12:20:08.708Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad 2. 2025-12-madebygather_bella_us_national_359.csv | Modified: 2025-12-31 08:53:50 UTC | Size: 9,316 bytes
2026-01-08T17:50:08.724+05:30
[INFO] 2026-01-08T12:20:08.724Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad File freshness validated: Source file (2026-01-08) is newer than last processed (2025-12-11)
2026-01-08T17:50:08.724+05:30
[INFO] 2026-01-08T12:20:08.724Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Processing: tracer/2026-01-madebygather_bella_us_national_359.csv
2026-01-08T17:50:08.756+05:30
[WARNING] 2026-01-08T12:20:08.756Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Skipping invalid file tracer/2026-01-madebygather_bella_us_national_359.csv: Sales (or Retailer_Sales)
2026-01-08T17:50:08.805+05:30
[INFO] 2026-01-08T12:20:08.805Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ✓ Transfer log written
2026-01-08T17:50:08.881+05:30
[INFO] 2026-01-08T12:20:08.881Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ✓ Audit log written to S3
2026-01-08T17:50:08.881+05:30
[INFO] 2026-01-08T12:20:08.881Z 7d60ff11-99d5-41a8-be96-8bead0e1f6ad ✓ Lambda 2 completed: 0 files, 0 records
2026-01-08T17:50:08.886+05:30
END RequestId: 7d60ff11-99d5-41a8-be96-8bead0e1f6ad
2026-01-08T17:50:08.886+05:30
REPORT RequestId: 7d60ff11-99d5-41a8-be96-8bead0e1f6ad Duration: 345.23 ms Billed Duration: 3390 ms Memory Size: 2048 MB Max Memory Used: 202 MB Init Duration: 3044.41 ms
