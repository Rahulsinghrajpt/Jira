Investigating the date column transformation logic. Reviewing the code that handles date/week column processing:
Reviewing the exact logic that processes week columns:
Checking if there's a standardization function that also processes week columns:
Checking the column standardizer for week column handling:
Checking the standardization function for week column processing:
Problem
The code in process_with_pandas (lines 687-710) finds any column containing "week" and renames the first match to "Date". This causes:
Columns like primeday_week, blackfriday_week are matched
The first match is renamed to "Date", creating a duplicate if "Date" already exists
Other "week" columns may be processed incorrectly
Root cause
# Line 688: This matches ANY column containing "week"week_columns_lower = [col for col in df_columns_lower if 'week' in col]
This matches:
week ✓ (intended)
primeday_week ✗ (unintended)
blackfriday_week ✗ (unintended)
Fix
Update the logic to:
Check if "Date" already exists — if yes, skip week processing
Only match a column named exactly "week" (case-insensitive), not columns containing "week"
Here's the corrected code:
# Around line 687-710 in lambda_function.py# Preserve original column names (data integrity requirement)original_columns = df.columns.tolist()# Create lowercase copy for checking only (DO NOT modify df.columns)df_columns_lower = df.columns.str.lower()# Check if "Date" column already existshas_date_column = 'date' in df_columns_lower# Only process week column if Date doesn't already existif not has_date_column:    # Find column named exactly "week" (case-insensitive), not columns containing "week"    week_col_index = None    original_week_col = None        for idx, col_lower in enumerate(df_columns_lower):        if col_lower == 'week':  # Exact match only            week_col_index = idx            original_week_col = original_columns[idx]            break        if week_col_index is not None and original_week_col:        logger.info(f"Found week column: {original_week_col}, renaming to 'Date'")        df = df.rename(columns={original_week_col: 'Date'})                # Update original_columns and df_columns_lower to reflect the rename        original_columns[week_col_index] = 'Date'        df_columns_lower = df.columns.str.lower()  # Recreate lowercase copy after rename                # Reformat date values to YYYY-MM-DD        try:            # Try parsing as date first            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')            # Format to YYYY-MM-DD            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')            # Replace NaT with empty string            df['Date'] = df['Date'].fillna('')            logger.info(f"Reformatted {len(df)} date values to YYYY-MM-DD format")        except Exception as e:            logger.warning(f"Error reformatting dates: {e}")    else:        logger.info("No 'week' column found (exact match), Date column may already exist or not needed")else:    logger.info("Date column already exists, skipping week column processing")
Changes
Check for existing "Date" column first
Match only exact "week" (case-insensitive), not columns containing "week"
Skip processing if "Date" already exists
Add logging for clarity
Testing
After applying the fix, verify:
Files with "Date" column → no duplicate Date columns
Files with "week" column → renamed to "Date"
Files with "primeday_week", "blackfriday_week" → left unchanged
Files with both "Date" and "week" → "Date" kept, "week" not processed
Should I create a patch file with this fix, or do you want to apply it directly?















Step 1: Create Lambda Zip Package
# Navigate to Lambda function directorycd C:\Users\anish_ksc\Desktop\local_repo\MikGorilla.AI\data_ingestion_pipeline\lambdas\mmm_dev_data_transfer# Create zip package with updated codeCompress-Archive -Path *.py -DestinationPath mmm_dev_data_transfer.zip -Force# Verify zip was createdTest-Path mmm_dev_data_transfer.zip# Should return: True
Step 2: Navigate to Terraform Directory
# Navigate to terraform directorycd C:\Users\anish_ksc\Desktop\local_repo\MikGorilla.AI\data_ingestion_pipeline\terraform
Step 3: Verify Terraform Workspace
# Check current workspace (should be 'default' for dev)terraform workspace show# If not 'default', switch to itterraform workspace select default
Step 4: Refresh AWS Credentials (if needed)
# Check if credentials are validaws sts get-caller-identity# If you get "ExpiredToken" error, refresh credentials:# Option 1: AWS SSOaws sso login --profile your-profile-name# Option 2: Set environment variables$env:AWS_ACCESS_KEY_ID = "your-access-key"$env:AWS_SECRET_ACCESS_KEY = "your-secret-key"$env:AWS_SESSION_TOKEN = "your-session-token"  # If using temporary credentials
Step 5: Validate Terraform Configuration
# Validate syntaxterraform validate# Expected output: Success! The configuration is valid.
Step 6: Plan Deployment
# Generate deployment plan.\deploy.ps1 -Environment dev -Action plan
Review the plan:
aws_lambda_function.data_transfer should show source_code_hash change
aws_lambda_alias.data_transfer_live should show version update
No other resources should change
Expected output:
Plan: 0 to add, 2 to change, 0 to destroy.
Step 7: Apply Deployment
# Deploy changes to dev environment.\deploy.ps1 -Environment dev -Action apply
During apply:
Review the plan when prompted
Type yes to confirm
Wait for completion (typically 1-2 minutes)
Expected output:
Apply complete! Resources: 0 added, 2 changed, 0 destroyed.
Step 8: Verify Lambda Deployment
# Check Lambda function was updatedaws lambda get-function --function-name mmm_dev_data_transfer --query 'Configuration.{Version:Version, LastModified:LastModified}' --output table# Expected: LastModified should show current timestamp
Step 9: Trigger Data Ingestion Pipeline
Option A: Trigger via Step Function (Recommended)
# Get Step Function ARN for dev$sfnArn = "arn:aws:states:eu-west-1:931493483974:stateMachine:mmm_dev_data_ingestion_orchestrator"# Start execution$execution = aws stepfunctions start-execution `    --state-machine-arn $sfnArn `    --input '{}' | ConvertFrom-Json# Save execution ARN$executionArn = $execution.executionArn








The Solution: "Query-Before-Failure"
Instead of guessing the key format, we treat DynamoDB as the Source of Truth.

Step 1: Try Standard Format – Attempt a direct lookup using the "ideal" normalized key (e.g., bella_us#amazon).

Step 2: Discover Real Format – If Step 1 fails, Query DynamoDB using the client_id to see all records for that client.

Step 3: Case-Insensitive Matching – Scan those records to find the brand and retailer (e.g., matching bella-US even if we searched for bella_us).

Step 4: Use the "True" Key – Extract the exact key format found in the database and use it to complete the request.
